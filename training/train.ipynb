{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import joblib\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/976142275983112277', creation_time=1741888125188, experiment_id='976142275983112277', last_update_time=1741888125188, lifecycle_stage='active', name='pytorch', tags={}>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "mlflow.set_experiment('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "resize_fact = 128\n",
    "batch_size = 20\n",
    "num_epochs = 50\n",
    "modelname = 'model4'\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transforms_list = transforms.Compose([\n",
    "    transforms.Resize((resize_fact,resize_fact)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root='../data/train/',transform=transforms_list)\n",
    "val_data = torchvision.datasets.ImageFolder(root='../data/val/',transform=transforms_list)\n",
    "test_data = torchvision.datasets.ImageFolder(root='../data/test/',transform=transforms_list)\n",
    "\n",
    "train_data_loader = data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "val_data_loader = data.DataLoader(val_data,batch_size=batch_size,shuffle=True)\n",
    "test_data_loader = data.DataLoader(test_data,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(resize_fact*resize_fact*3,84)\n",
    "        self.fc2 = nn.Linear(84,50)\n",
    "        self.fc3 = nn.Linear(50,2)\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,resize_fact*resize_fact*3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # Since we are going to use CrossEntropyLoss, we don't need to apply softmax here, bcoz it is already applied in the loss function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(resize_fact*resize_fact*3,200)\n",
    "        self.fc2 = nn.Linear(200,100)\n",
    "        self.fc3 = nn.Linear(100,50)\n",
    "        self.fc4 = nn.Linear(50,2)\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,resize_fact*resize_fact*3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x) # Since we are going to use CrossEntropyLoss, we don't need to apply softmax here, bcoz it is already applied in the loss function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(resize_fact*resize_fact*3,200)\n",
    "        self.fc2 = nn.Linear(200,100)\n",
    "        self.fc3 = nn.Linear(100,50)\n",
    "        self.fc4 = nn.Linear(50,2)\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,resize_fact*resize_fact*3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x,0.5)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x) # Since we are going to use CrossEntropyLoss, we don't need to apply softmax here, bcoz it is already applied in the loss function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(resize_fact*resize_fact*3,300)\n",
    "        self.fc2 = nn.Linear(300,100)\n",
    "        self.fc3 = nn.Linear(100,50)\n",
    "        self.fc4 = nn.Linear(50,2)\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,resize_fact*resize_fact*3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x)) \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x) # Since we are going to use CrossEntropyLoss, we don't need to apply softmax here, bcoz it is already applied in the loss function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelname=='model1':\n",
    "    model=Model1()\n",
    "elif modelname=='model2':\n",
    "    model=Model2()\n",
    "elif modelname=='model3':\n",
    "    model=Model3()\n",
    "elif modelname=='model4':\n",
    "    model=Model4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model4(\n",
       "  (fc1): Linear(in_features=49152, out_features=300, bias=True)\n",
       "  (fc2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (fc4): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "def train_model(model, optimizer,loss_fun,train_loader,val_loader,epochs=20,device='gpu'):\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs,targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fun(outputs,targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        \n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        writer.add_scalar('Loss/train',training_loss,epoch)\n",
    "\n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs,targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fun(outputs,targets)\n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(outputs,dim=1),dim=1)[1],targets).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "        val_accuracy= num_correct/num_examples\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch,training_loss,valid_loss,num_correct/num_examples))\n",
    "    return model,val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.84, Validation Loss: 0.70, accuracy = 0.75\n",
      "Epoch: 1, Training Loss: 0.42, Validation Loss: 0.90, accuracy = 0.61\n",
      "Epoch: 2, Training Loss: 0.34, Validation Loss: 0.56, accuracy = 0.71\n",
      "Epoch: 3, Training Loss: 0.23, Validation Loss: 0.47, accuracy = 0.79\n",
      "Epoch: 4, Training Loss: 0.17, Validation Loss: 0.89, accuracy = 0.75\n",
      "Epoch: 5, Training Loss: 0.12, Validation Loss: 0.74, accuracy = 0.79\n",
      "Epoch: 6, Training Loss: 0.17, Validation Loss: 0.97, accuracy = 0.64\n",
      "Epoch: 7, Training Loss: 0.16, Validation Loss: 0.89, accuracy = 0.75\n",
      "Epoch: 8, Training Loss: 0.27, Validation Loss: 1.38, accuracy = 0.75\n",
      "Epoch: 9, Training Loss: 0.22, Validation Loss: 0.66, accuracy = 0.68\n",
      "Epoch: 10, Training Loss: 0.21, Validation Loss: 1.02, accuracy = 0.71\n",
      "Epoch: 11, Training Loss: 0.06, Validation Loss: 1.42, accuracy = 0.71\n",
      "Epoch: 12, Training Loss: 0.12, Validation Loss: 1.22, accuracy = 0.71\n",
      "Epoch: 13, Training Loss: 0.09, Validation Loss: 1.13, accuracy = 0.79\n",
      "Epoch: 14, Training Loss: 0.02, Validation Loss: 0.89, accuracy = 0.82\n",
      "Epoch: 15, Training Loss: 0.00, Validation Loss: 0.91, accuracy = 0.79\n",
      "Epoch: 16, Training Loss: 0.00, Validation Loss: 0.89, accuracy = 0.79\n",
      "Epoch: 17, Training Loss: 0.00, Validation Loss: 0.92, accuracy = 0.79\n",
      "Epoch: 18, Training Loss: 0.00, Validation Loss: 0.94, accuracy = 0.79\n",
      "Epoch: 19, Training Loss: 0.00, Validation Loss: 0.95, accuracy = 0.79\n",
      "Epoch: 20, Training Loss: 0.00, Validation Loss: 0.98, accuracy = 0.79\n",
      "Epoch: 21, Training Loss: 0.00, Validation Loss: 0.99, accuracy = 0.79\n",
      "Epoch: 22, Training Loss: 0.00, Validation Loss: 1.00, accuracy = 0.79\n",
      "Epoch: 23, Training Loss: 0.00, Validation Loss: 1.01, accuracy = 0.79\n",
      "Epoch: 24, Training Loss: 0.00, Validation Loss: 1.02, accuracy = 0.79\n",
      "Epoch: 25, Training Loss: 0.00, Validation Loss: 1.03, accuracy = 0.79\n",
      "Epoch: 26, Training Loss: 0.00, Validation Loss: 1.04, accuracy = 0.79\n",
      "Epoch: 27, Training Loss: 0.00, Validation Loss: 1.05, accuracy = 0.79\n",
      "Epoch: 28, Training Loss: 0.00, Validation Loss: 1.06, accuracy = 0.79\n",
      "Epoch: 29, Training Loss: 0.00, Validation Loss: 1.07, accuracy = 0.79\n",
      "Epoch: 30, Training Loss: 0.00, Validation Loss: 1.08, accuracy = 0.79\n",
      "Epoch: 31, Training Loss: 0.00, Validation Loss: 1.09, accuracy = 0.79\n",
      "Epoch: 32, Training Loss: 0.00, Validation Loss: 1.10, accuracy = 0.79\n",
      "Epoch: 33, Training Loss: 0.00, Validation Loss: 1.11, accuracy = 0.79\n",
      "Epoch: 34, Training Loss: 0.00, Validation Loss: 1.12, accuracy = 0.79\n",
      "Epoch: 35, Training Loss: 0.00, Validation Loss: 1.12, accuracy = 0.79\n",
      "Epoch: 36, Training Loss: 0.00, Validation Loss: 1.13, accuracy = 0.79\n",
      "Epoch: 37, Training Loss: 0.00, Validation Loss: 1.14, accuracy = 0.79\n",
      "Epoch: 38, Training Loss: 0.00, Validation Loss: 1.15, accuracy = 0.79\n",
      "Epoch: 39, Training Loss: 0.00, Validation Loss: 1.16, accuracy = 0.79\n",
      "Epoch: 40, Training Loss: 0.00, Validation Loss: 1.17, accuracy = 0.79\n",
      "Epoch: 41, Training Loss: 0.00, Validation Loss: 1.17, accuracy = 0.79\n",
      "Epoch: 42, Training Loss: 0.00, Validation Loss: 1.18, accuracy = 0.79\n",
      "Epoch: 43, Training Loss: 0.00, Validation Loss: 1.19, accuracy = 0.79\n",
      "Epoch: 44, Training Loss: 0.00, Validation Loss: 1.19, accuracy = 0.79\n",
      "Epoch: 45, Training Loss: 0.00, Validation Loss: 1.20, accuracy = 0.79\n",
      "Epoch: 46, Training Loss: 0.00, Validation Loss: 1.20, accuracy = 0.79\n",
      "Epoch: 47, Training Loss: 0.00, Validation Loss: 1.21, accuracy = 0.79\n",
      "Epoch: 48, Training Loss: 0.00, Validation Loss: 1.22, accuracy = 0.79\n",
      "Epoch: 49, Training Loss: 0.00, Validation Loss: 1.22, accuracy = 0.79\n"
     ]
    }
   ],
   "source": [
    "trained_model,val_accuracy = train_model(model, optimizer,nn.CrossEntropyLoss(), train_data_loader,val_data_loader, epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()\n",
    "valid_loss=0.0\n",
    "num_correct = 0\n",
    "num_examples = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        inputs,targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = trained_model(inputs)\n",
    "        loss = loss_fun(outputs,targets)\n",
    "        valid_loss += loss.data.item() * inputs.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(outputs,dim=1),dim=1)[1],targets).view(-1)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "    valid_loss /= len(test_data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.79\n",
      "Test Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "print('Val Accuracy: {:.2f}'.format(val_accuracy))\n",
    "test_accuracy = num_correct/num_examples\n",
    "print('Test Accuracy: {:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/13 19:14:31 WARNING mlflow.utils.requirements_utils: Found torch version (2.2.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.2.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/03/13 19:14:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run trusting-whale-381 at: http://127.0.0.1:8080/#/experiments/976142275983112277/runs/361ad0a67d504c40b70e79854337c722\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/976142275983112277\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    mlflow.log_param('resize_fact',resize_fact)\n",
    "    mlflow.log_param('batch_size',batch_size)\n",
    "    mlflow.log_param('num_epochs',num_epochs)\n",
    "    mlflow.log_param('modelname',modelname)\n",
    "    mlflow.log_param('learning_rate',learning_rate)\n",
    "    mlflow.log_metric('val_accuracy',val_accuracy)\n",
    "    mlflow.log_metric('test_accuracy',test_accuracy)\n",
    "    mlflow.pytorch.log_model(trained_model,'model')\n",
    "    joblib.dump(trained_model,'../models/model.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
